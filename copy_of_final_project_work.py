# -*- coding: utf-8 -*-
"""Copy of Final Project Work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16QhQp_ItF0lscz6qMWVttupCsO55nk5s
"""

#Maya Johnson
#41721895
#This is my progress so far on my final project!
#I still need to work on countouring my images. I also need to work on making my linear regression model accurate and my KNN neighbors plot more applicable. Finally, I need to figure out which functions I will need to write tests for besides the two I already have!
#Link to Blood Cell Dataset: https://www.kaggle.com/datasets/akhiljethwa/blood-cancer-image-dataset?resource=download
#datasetlink: https://www.kaggle.com/datasets/utkarshx27/breast-cancer-wisconsin-diagnostic-dataset

import numpy as np
import pandas as pd

from google.colab.patches import cv2_imshow  #COMMENT OUT IF USING GRADESCOPE
from scipy.stats import shapiro, ttest_ind, f_oneway
from scipy.optimize import curve_fit
from google.colab import drive #COMMENT OUT IF USING GRADESCOPE
drive.mount('/content/drive') #COMMENT OUT IF USING GRADESCOPE
import cv2
from google.colab.patches import cv2_imshow  #COMMENT OUT IF USING GRADESCOPE

from sklearn import metrics

import scipy as sp
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import f1_score
from sklearn.metrics import precision_score

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import HistGradientBoostingClassifier

! pip install kneed
from kneed import KneeLocator
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

#IMAGES

def contourFinder(fileName, lowerBound):
  """This function takes in the path of an image file and a lower bound for thresholding,
  and inverts the image and turns it to grayscale, then it returns a list of contours for the cells in the image."""

  imageColor = cv2.imread(fileName)   # Stores the colored version of the image

  image = cv2.imread(fileName, cv2.IMREAD_GRAYSCALE)   # convert the image to grayscale

  inverted = np.invert(image)

  ret, thresh = cv2.threshold(inverted, lowerBound, 255, cv2.THRESH_BINARY)   # Makes the image very clear using a threshold
  contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  # Finds the contour of the gray, thresholded image
  print('Contour Count:', len(contours))

  contoursImage = cv2.drawContours(imageColor, contours, -1, (0,255,255), 2)
  cv2_imshow(contoursImage)

  return contours

def areaFinder(contours):
  areas = []
  for contour in contours:
    area = cv2.contourArea(contour)   # Finds the area of a contour using the contours list
    areas.append(area)

  return areas    # Returns the area

def maxAreaFinder(areas):

  maxArea = max(areas)
  if maxArea > 0:
    return maxArea
  else:
      print("Invalid max area found.")
      return None

def averageFinder(areas):
  total = sum(areas)
  averageArea = total / len(areas)
  return averageArea

def cleanAreaList(areas, percentile):

  numberToRemoveBottom = int(len(areas) * percentile)
  numberToRemoveTop = int(len(areas) * 0.01)
  sortedList = sorted(areas)
  result = sortedList[numberToRemoveBottom:-numberToRemoveTop]

  print('Cleaned Contour Count:', len(result))

  return result

def main():

  image1 = '/content/drive/MyDrive/ImagesDataset/101B.PNG'   # Use the first image as an example
  image2 = '/content/drive/MyDrive/ImagesDataset/101M.PNG'

  #image1 = '101B.PNG'
  #image2 = '101M.PNG'

  contours1 = contourFinder(image1, 130)      # Call the contourFinder function
  contours2 = contourFinder(image2, 125)      # Call the contourFinder function

  areas1 = areaFinder(contours1)     # areaFinder function
  areas2 = areaFinder(contours2)     # areaFinder function

  cleanedAreas1 = cleanAreaList(areas1, .25)
  cleanedAreas2 = cleanAreaList(areas2, .50)

  averageAreaB = averageFinder(cleanedAreas1)
  averageAreaM = averageFinder(cleanedAreas2)

  print('Average Area of Benign Nuclei:', averageAreaB)
  print('Average Area of Malignant Nuclei:', averageAreaM)

  maxAreaB = maxAreaFinder(cleanedAreas1)
  maxAreaM = maxAreaFinder(cleanedAreas2)

  print('Maximum Area of Benign Nuclei:', maxAreaB)
  print('Maximum Area of Malignant Nuclei:', maxAreaM)

if __name__ == "__main__":
  main()

# imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, f1_score
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


#DATASET
def readData(dataSheet):
    """This function reads data from the attached spreadsheet, specifying the sheetname, and converts it into a dataframe."""
    df = pd.read_excel(dataSheet)   # Read data from the spreadsheet link
    return df   # Return the dataframe

def train_model(df):

    if isinstance(df, pd.DataFrame):
      print("Input is a dataframe.")

    df.drop(["Unnamed: 0"], axis=1, inplace=True)
    df.dropna(inplace=True)  # Drop rows with NaN values

    if len(df) < 2:         # Check if the dataset has enough samples
        raise ValueError("Dataset does not contain enough samples.")
    y = df['malignancy'].values
    x_data = df.drop(["malignancy"], axis=1)
    x = (x_data - x_data.min()) / (x_data.max() - x_data.min()).values  # Normalize the data
    # Train-test split
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)
    if len(x_train) == 0 or len(x_test) == 0:
        raise ValueError("Train or test set is empty. Adjust train-test split parameters.")

    return x_train.T, y_train.T, x_test.T, y_test.T

def logisticRegression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):
    dimension = x_train.shape[0]
    lr = LogisticRegression()
    lr.fit(x_train.T, y_train.T)
    testAccuracy = lr.score(x_test.T, y_test.T) * 100
    yPred = lr.predict(x_test.T)
    print("Test accuracy: {} %".format(lr.score(x_test.T, y_test.T) * 100))

    # Scores
    print("Training score:", lr.score(x_train.T, y_train.T)) # Print training score
    print("Testing score:", lr.score(x_test.T, y_test.T)) # Print testing score
    print("Precision score:", precision_score(y_test.T, yPred, average='binary')) # Print precision score
    print("F1 score:", f1_score(y_test.T, yPred, average='binary')) # Print F1 score
    cnf_matrix = confusion_matrix(y_test.T, yPred) # Generate confusion matrix
    classNames = ["Benign", "Malignant"] # Define class names for confusion matrix
    cnf_matrix = pd.DataFrame(cnf_matrix, index=classNames, columns=classNames) # Create dataframe for the confusion matrix
    plt.figure(figsize=(8, 6)) # Set figure size for the plot
    #sns.heatmap(cnf_matrix, annot=True, cmap="prism", fmt='g', xticklabels=classNames, yticklabels=classNames) # Plot the confusion matrix
    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g', xticklabels=classNames, yticklabels=classNames) # Creating heatmap
    plt.xlabel('Predicted') # Set x-label
    plt.ylabel('Actual') # Set y-label
    plt.title('Confusion Matrix') # Set plot title
    plt.tight_layout() # Adjust layout
    plt.show() # Show the plot

def advancedVisualization(df): # Function for advanced visualization using the dataframe
    sns.boxplot(x='malignancy', y='x.radius_mean', data=df) # Create a box plot to show platelets distribution by smoking status
    plt.title('Average Cell Radius Distribution by Malignancy Status') # Set box plot title
    plt.show() # Show the box plot

def knnAndPlot(df): # Perform KNN analysis with decision boundary plotting

    X = df[['x.radius_mean', 'x.texture_mean']].values    # Set X values as 'age' and 'serum_creatinine' columns for visualization
    y = df['malignancy'].values # Set y values as the 'malignancy' column
    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.25, random_state=42) # Split the dataset

    knn = KNeighborsClassifier(n_neighbors=5)  # Initialize the k-NN classifier
    knn.fit(xTrain, yTrain) # Fit the model

    xMin, xMax = xTrain[:, 0].min() - 1, xTrain[:, 0].max() + 1     # Define ranges for decision boundary plotting
    yMin, yMax = xTrain[:, 1].min() - 1, xTrain[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(xMin, xMax, 0.1), # Create a mesh grid for plotting
                         np.arange(yMin, yMax, 0.1))

    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]) # Predict all points on the grid
    Z = Z.reshape(xx.shape) # Reshape the predictions
    plt.contourf(xx, yy, Z, alpha=0.4)     # Plot the decision boundaries
    sns.scatterplot(x=xTrain[:, 0], y=xTrain[:, 1], hue=yTrain,     # Plot the training samples
                    palette='prism', edgecolor='k', s=20)

    plt.title('k-NN Decision Boundaries with Training Samples') # Set plot title
    plt.xlabel('Average Smoothness') # Set x-label
    plt.ylabel('Average Radius') # Set y-label
    plt.legend(title='Malignancy', loc='upper right') # Set legend
    plt.show() # Show the plot

def forestClassifierModel(df):
    """Train a RandomForestClassifier model on the data to predict patient outcome. Returns the trained model."""

    X = df.drop(columns=['malignancy'])  # Extract features from the dataframe
    y = df['malignancy']  # Extract target variable

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize and train the RandomForestClassifier model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    return model  # Return the trained model

def featureImportance(model, features):
    """Compute and visualize feature importances from a trained GradientBoostingClassifier model. This function returns the trained GradientBoostingClassifier model.
    It generates a histogram comparing each column of the dataframe and its predicted influence on heart failure."""

    importances = model.feature_importances_  # Extract feature importances from the model
    indices = sorted(range(len(importances)), key=lambda i: importances[i], reverse=True)[1:]  # Sort feature importances and exclude the highest importance

    # Plotting feature importances
    plt.figure(figsize=(10, 6))
    plt.title("Feature Importances")
    plt.bar(range(len(indices)), [importances[i] for i in indices], align="center")  # Create a bar plot excluding the highest importance
    plt.xticks(range(len(indices)), [features[i] for i in indices], rotation=90)  # Set x-axis labels
    plt.xlabel("Feature")  # Set x-axis label
    plt.ylabel("Importance")  # Set y-axis label
    plt.tight_layout()
    plt.show()  # Display the plot

def main(): # Main function
    dataset = '/content/drive/MyDrive/Dataset/Excel2.xlsx'
    df = readData(dataset) # Read data from the provided spreadsheet and convert to dataframe
    x_train, y_train, x_test, y_test = train_model(df)
    logisticRegression(x_train, y_train, x_test, y_test, learning_rate=3, num_iterations=300)

    knnAndPlot(df)     # Perform KNN analysis and plot decision boundaries
    model = forestClassifierModel(df) # Train model
    features = df.columns[:-1]    # Get feature names
    featureImportance(model, features)   # Compute and visualize feature importance
    advancedVisualization(df)     # Perform advanced visualization

if __name__ == "__main__":
    main()

from typing import ValuesView
import unittest

# UNITTESTS FOR IMAGES FUNCTIONS

class TestImages(unittest.TestCase):

 # Tests Average Contour Finder function
  def test_MaxAreaFinder(self):
    values1 = [10,20,30,40]
    result = maxAreaFinder(values1)
    self.assertEqual(result, 40)

    values2 = [30,50,0,50]
    result = maxAreaFinder(values2)
    self.assertEqual(result, 50)

 # Tests averageFinder function
  def test_averageFinder(self):
    values1 = [2, 4, 6]
    result = averageFinder(values1)
    self.assertEqual(result, 4)

    values2 = [-5,-7]
    result = averageFinder(values2)
    self.assertEqual(result, -6)

  # Tests the cleanAreasList function
  def test_cleanAreaList(self):
    values1 = [5, 3, 8, 1, 2, 4, 7, 6, 9, 10]
    result = cleanAreaList(values1, 0.25)
    self.assertEqual(result, [3, 4, 5, 6, 7, 8, 9])

    values3 = []
    result = cleanAreaList(values2, 0.75)
    self.assertEqual(result, [])

    values2 = [5,5,5,5,5,5,5,5,5,5]
    result = cleanAreaList(values2, 0.25)
    self.assertEqual(result, [5,5,5,5,5])

unittest.main(argv=[''], verbosity=2, exit=False)
