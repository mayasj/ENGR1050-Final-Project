# -*- coding: utf-8 -*-
"""Final Project Work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BtM_2-laDwA89y_aDnMY8Nhp6VityLVs
"""

#Maya Johnson
#41721895
#This is my progress so far on my final project!
#I still need to work on countouring my images. I also need to work on making my linear regression model accurate and my KNN neighbors plot more applicable. Finally, I need to figure out which functions I will need to write tests for besides the two I already have!
#Link to Blood Cell Dataset: https://www.kaggle.com/datasets/akhiljethwa/blood-cancer-image-dataset?resource=download
#datasetlink: https://www.kaggle.com/datasets/utkarshx27/breast-cancer-wisconsin-diagnostic-dataset

import numpy as np
import pandas as pd

from google.colab.patches import cv2_imshow  #COMMENT OUT IF USING GRADESCOPE
from scipy.stats import shapiro, ttest_ind, f_oneway
from scipy.optimize import curve_fit
from google.colab import drive #COMMENT OUT IF USING GRADESCOPE
drive.mount('/content/drive') #COMMENT OUT IF USING GRADESCOPE
import cv2
from google.colab.patches import cv2_imshow  #COMMENT OUT IF USING GRADESCOPE

from sklearn import metrics

import scipy as sp
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import f1_score
from sklearn.metrics import precision_score

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import HistGradientBoostingClassifier

! pip install kneed
from kneed import KneeLocator
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

#IMAGES

def contourFinder(fileName, lowerBound):
    """This function takes in the path of an image file and a lower bound for thresholding,
    and inverts the image and turns it to grayscale, then it returns a list of contours for the cells in the image."""

    imageColor = cv2.imread(fileName)   # Stores the colored version of the image

    image = cv2.imread(fileName, cv2.IMREAD_GRAYSCALE)   # convert the image to grayscale

    inverted = np.invert(image)

    ret, thresh = cv2.threshold(inverted, lowerBound, 255, cv2.THRESH_BINARY)   # Makes the image very clear using a threshold
    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  # Finds the contour of the gray, thresholded image
    print(len(contours))

    contoursImage = cv2.drawContours(imageColor, contours, -1, (0,255,255), 2)
    cv2_imshow(contoursImage)

    return contours

def areaFinder(contours):
  areas = []
  for value in contours:
    area = cv2.contourArea(value)   # Finds the area of a contour using the contours list
    areas.append(area)

  return areas    # Returns the area

def centroidFinder(contours):

  M = cv2.moments(contours)   # finds the weighted avg of the pixels of the image
  if M["m00"] != 0:    # makes sure that the avg doessn't equal zero
    X = int(M["m10"] / M["m00"])   # x position of the centroid
    Y = int(M["m01"] / M["m00"])   # y position of the centroid
  centroid = (X, Y)   # return the centroid bounds as a tuple

  return centroid

def brightnessFinder(fileName, contours):

  image = cv2.imread(fileName, cv2.IMREAD_GRAYSCALE)    # Read the image in grayscale
  mask = np.zeros_like(image)     # Create a mask for the region bounded by the contours

  maskedImage = cv2.bitwise_and(image, mask)     # Compute the mean brightness of the region

  brightness = np.mean(maskedImage)   # Take the average

  return brightness

def maxAreaContourFinder(contours):
  maxArea = 0
  maxContour = None

  # Check if contours were found
  if contours:
      # Use a for loop to iterate through the list of contours and find their areas
      for contour in contours:
          area = cv2.contourArea(contour)
          if area > maxArea:
              maxArea = area
              maxContour = contour
      return maxContour    # Returns the contour with the maximum area
  else:
      print("No contours found.")
      return None

def averageContourFinder(contours):
  total = sum(contours)
  average = total / len(contours)
  return average

def velocityCalculator(centroid1, centroid2):

  x1, y1 = centroid1       # Unpacks the original centroid position
  x2, y2 = centroid2       # Unpacks the new centroid position
  xNew = x2 - x1
  yNew = y2 - y1
  distance = (xNew**2 + yNew**2)**0.5      # Distance formula to find distance between new and old positions
  velocity = distance / 600                # Finds the velocity in units/sec

  return velocity                          # Returns the velocity

"""def relChangeFinder(val1, val2):

  relativeChange = (val2 - val1) / val1    # Relative change formula

  return relativeChange                    # Returns relative change"""

"""def oneTrial(trialName, threshold):

  #image1 = trialName + 'B.PNG'       # Use this format for Gradescope
  #image2 = trialName + 'M.PNG'      # Use this for Gradescope
  image1 = "/content/drive/MyDrive/ImagesDataset/" + trialName + "B.PNG"      ##COMMENT OUT
  image2 = "/content/drive/MyDrive/ImagesDataset/" + trialName + "M.PNG"      ##COMMENT OUT

  contours1 = contourFinder(image1, threshold)     # Finds the list of contours
  contours2 = contourFinder(image2, threshold)

  maxContour1 = maxAreaContourFinder(contours1)    # Finds the maximum contour of the list
  maxContour2 = maxAreaContourFinder(contours2)

  print('Max Contour B:', maxContour1)
  print('Max Contour M:', maxContour2)

  centroid1 = centroidFinder(maxContour1)         # Finds the centroid of the maximum contour
  centroid2 = centroidFinder(maxContour2)

  brightness1 = brightnessFinder(image1, maxContour1)     # Finds the brightness of the maximum contour
  brightness2 = brightnessFinder(image2, maxContour2)

  area1 = areaFinder(maxContour1)                         # Finds the area of the maximum contour
  area2 = areaFinder(maxContour2)

  velocity = velocityCalculator(centroid1, centroid2)     # Finds the velocity

  relativeChangeA = relChangeFinder(area1, area2)         # Each relative change is in terms of area and brightness
  relativeChangeB = relChangeFinder(brightness1, brightness2)
  finalList = [trialName, velocity, relativeChangeA, relativeChangeB]     # Compiles the list of metrics

  return finalList"""

def dataWriter(contours1, contours2):       # Converts the nested list to an Excel spreadsheet

  list_count = [i for i in range(1,201)]
  contours1Short = contours1[:200]
  contours2Short = contours2[:200]

  df = pd.DataFrame({'Count': [list_count],'BenignContours': [contours1Short], 'MalignantContours': [contours2Short]})

  #with pd.ExcelWriter("outputFile.xlsx") as writer:
  #df.to_excel(writer, sheet_name = "output_sheet")      # Creates the sheet with the name output_sheet
  #print("Saved excel file")   # Confirmation that it saved the excel file

def main():
  image1 = '/content/drive/MyDrive/ImagesDataset/101B.PNG'   # Use the first image as an example
  image2 = '/content/drive/MyDrive/ImagesDataset/101M.PNG'

  contours1 = contourFinder(image1, 130)      # Call the contourFinder function
  contours2 = contourFinder(image2, 125)      # Call the contourFinder function

  area1 = areaFinder(contours1)     # areaFinder function
  area2 = areaFinder(contours2)     # areaFinder function

  dataWriter(contours1, contours2)

  #oneTrial('101', 120)     # Call the oneTrial function

  averageContourValueB = averageContourFinder(contours1)
  averageContourValueM = averageContourFinder(contours2)

  averageAreaB = averageContourFinder(area1)
  averageAreaM = averageContourFinder(area2)

  print('Average Contour Value of Benign Nuclei:', averageContourValueB)
  print('Average Contour Value of Malignant Nuclei:', averageContourValueM)

  print('Average Area of Benign Nuclei:', averageAreaB)
  print('Average Area of Malignant Nuclei:', averageAreaM)

  #print(centroidFinder(contour1))    # Print the centroid dimensions
  #print(centroidFinder(contour2))    # Print the centroid dimensions"""

if __name__ == "__main__":
  main()

# imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, f1_score
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


#DATASET
def readData(dataSheet):
    """This function reads data from the attached spreadsheet, specifying the sheetname, and converts it into a dataframe."""
    df = pd.read_excel(dataSheet)   # Read data from the spreadsheet link
    return df   # Return the dataframe

def train_model(df):

    if isinstance(df, pd.DataFrame):
      print("Input is a dataframe.")

    df.drop(["Unnamed: 0"], axis=1, inplace=True)
    df.dropna(inplace=True)  # Drop rows with NaN values

    if len(df) < 2:         # Check if the dataset has enough samples
        raise ValueError("Dataset does not contain enough samples.")
    y = df['malignancy'].values
    x_data = df.drop(["malignancy"], axis=1)
    x = (x_data - x_data.min()) / (x_data.max() - x_data.min()).values  # Normalize the data
    # Train-test split
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)
    if len(x_train) == 0 or len(x_test) == 0:
        raise ValueError("Train or test set is empty. Adjust train-test split parameters.")

    return x_train.T, y_train.T, x_test.T, y_test.T

def logisticRegression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):
    dimension = x_train.shape[0]
    lr = LogisticRegression()
    lr.fit(x_train.T, y_train.T)
    testAccuracy = lr.score(x_test.T, y_test.T) * 100
    yPred = lr.predict(x_test.T)
    print("Test accuracy: {} %".format(lr.score(x_test.T, y_test.T) * 100))

    # Scores
    print("Training score:", lr.score(x_train.T, y_train.T)) # Print training score
    print("Testing score:", lr.score(x_test.T, y_test.T)) # Print testing score
    print("Precision score:", precision_score(y_test.T, yPred, average='binary')) # Print precision score
    print("F1 score:", f1_score(y_test.T, yPred, average='binary')) # Print F1 score
    cnf_matrix = confusion_matrix(y_test.T, yPred) # Generate confusion matrix
    classNames = ["Benign", "Malignant"] # Define class names for confusion matrix
    cnf_matrix = pd.DataFrame(cnf_matrix, index=classNames, columns=classNames) # Create dataframe for the confusion matrix
    plt.figure(figsize=(8, 6)) # Set figure size for the plot
    #sns.heatmap(cnf_matrix, annot=True, cmap="prism", fmt='g', xticklabels=classNames, yticklabels=classNames) # Plot the confusion matrix
    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g', xticklabels=classNames, yticklabels=classNames) # Creating heatmap
    plt.xlabel('Predicted') # Set x-label
    plt.ylabel('Actual') # Set y-label
    plt.title('Confusion Matrix') # Set plot title
    plt.tight_layout() # Adjust layout
    plt.show() # Show the plot

def advancedVisualization(df): # Function for advanced visualization using the dataframe
    sns.boxplot(x='malignancy', y='x.radius_mean', data=df) # Create a box plot to show platelets distribution by smoking status
    plt.title('Average Cell Radius Distribution by Malignancy Status') # Set box plot title
    plt.show() # Show the box plot

def knnAndPlot(df): # Perform KNN analysis with decision boundary plotting

    X = df[['x.radius_mean', 'x.texture_mean']].values    # Set X values as 'age' and 'serum_creatinine' columns for visualization
    y = df['malignancy'].values # Set y values as the 'malignancy' column
    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.25, random_state=42) # Split the dataset

    knn = KNeighborsClassifier(n_neighbors=5)  # Initialize the k-NN classifier
    knn.fit(xTrain, yTrain) # Fit the model

    xMin, xMax = xTrain[:, 0].min() - 1, xTrain[:, 0].max() + 1     # Define ranges for decision boundary plotting
    yMin, yMax = xTrain[:, 1].min() - 1, xTrain[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(xMin, xMax, 0.1), # Create a mesh grid for plotting
                         np.arange(yMin, yMax, 0.1))

    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]) # Predict all points on the grid
    Z = Z.reshape(xx.shape) # Reshape the predictions
    plt.contourf(xx, yy, Z, alpha=0.4)     # Plot the decision boundaries
    sns.scatterplot(x=xTrain[:, 0], y=xTrain[:, 1], hue=yTrain,     # Plot the training samples
                    palette='prism', edgecolor='k', s=20)

    plt.title('k-NN Decision Boundaries with Training Samples') # Set plot title
    plt.xlabel('Average Smoothness') # Set x-label
    plt.ylabel('Average Radius') # Set y-label
    plt.legend(title='Malignancy', loc='upper right') # Set legend
    plt.show() # Show the plot

def forestClassifierModel(df):
    """Train a RandomForestClassifier model on the data to predict patient outcome. Returns the trained model."""

    X = df.drop(columns=['malignancy'])  # Extract features from the dataframe
    y = df['malignancy']  # Extract target variable

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize and train the RandomForestClassifier model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    return model  # Return the trained model

def featureImportance(model, features):
    """Compute and visualize feature importances from a trained GradientBoostingClassifier model. This function returns the trained GradientBoostingClassifier model.
    It generates a histogram comparing each column of the dataframe and its predicted influence on heart failure."""

    importances = model.feature_importances_  # Extract feature importances from the model
    indices = sorted(range(len(importances)), key=lambda i: importances[i], reverse=True)[1:]  # Sort feature importances and exclude the highest importance

    # Plotting feature importances
    plt.figure(figsize=(10, 6))
    plt.title("Feature Importances")
    plt.bar(range(len(indices)), [importances[i] for i in indices], align="center")  # Create a bar plot excluding the highest importance
    plt.xticks(range(len(indices)), [features[i] for i in indices], rotation=90)  # Set x-axis labels
    plt.xlabel("Feature")  # Set x-axis label
    plt.ylabel("Importance")  # Set y-axis label
    plt.tight_layout()
    plt.show()  # Display the plot

def main(): # Main function
    dataset = '/content/drive/MyDrive/Dataset/Excel2.xlsx'
    df = readData(dataset) # Read data from the provided spreadsheet and convert to dataframe
    x_train, y_train, x_test, y_test = train_model(df)
    logisticRegression(x_train, y_train, x_test, y_test, learning_rate=3, num_iterations=300)

    knnAndPlot(df)     # Perform KNN analysis and plot decision boundaries
    model = forestClassifierModel(df) # Train model
    features = df.columns[:-1]    # Get feature names
    featureImportance(model, features)   # Compute and visualize feature importance
    advancedVisualization(df)     # Perform advanced visualization

if __name__ == "__main__":
    main()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

def readData(dataSheet):
    """This function reads data from the attached spreadsheet, specifying the sheetname, and converts it into a dataframe."""
    df = pd.read_excel(dataSheet)   # Read data from the spreadsheet link
    return df   # Return the dataframe

def trainModel(df):
    df.drop(["Unnamed: 0"], axis=1, inplace=True)
    df.dropna(inplace=True)  # Drop rows with NaN values
    # Check if the dataset has enough samples
    if len(df) < 2:
        raise ValueError("Dataset does not contain enough samples.")
    y = df['malignancy'].values
    x_data = df.drop(["malignancy"], axis=1)
    x = (x_data - x_data.min()) / (x_data.max() - x_data.min()).values  # Normalize the data
    # Train-test split
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    if len(x_train) == 0 or len(x_test) == 0:
        raise ValueError("Train or test set is empty. Adjust train-test split parameters.")
    return x_train.T, y_train.T, x_test.T, y_test.T

def logisticRegression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):
    dimension = x_train.shape[0]
    lr = LogisticRegression()
    lr.fit(x_train.T, y_train.T)
    print("Test accuracy: {} %".format(lr.score(x_test.T, y_test.T) * 100))

def main(dataSheet):
    df = readData(dataSheet)
    x_train, y_train, x_test, y_test = trainModel(df)
    logisticRegression(x_train, y_train, x_test, y_test, learning_rate=3, num_iterations=300)

if __name__ == "__main__":
    dataSheet = "/content/drive/MyDrive/Dataset/Excel2.xlsx"
    main(dataSheet)

import unittest

class TestFunctions(unittest.TestCase):

    def setUp(self):
        self.dataset_path = '/content/drive/MyDrive/Dataset/Excel2.xlsx'

    def test_readData(self):
        df = readData(self.dataset_path)
        self.assertIsInstance(df, pd.DataFrame)  # Check if the output is a DataFrame
        self.assertFalse(df.empty)  # Check if the DataFrame is not empty

    def test_train_model(self):
        df = readData(self.dataset_path)
        x_train, y_train, x_test, y_test = train_model(df)
        self.assertEqual(x_train.shape[0], y_train.shape[0])  # Check if the number of samples in x_train and y_train match
        self.assertEqual(x_test.shape[0], y_test.shape[0])    # Check if the number of samples in x_test and y_test match

    def test_logisticRegression(self):
        df = readData(self.dataset_path)
        x_train, y_train, x_test, y_test = train_model(df)
        test_accuracy = logisticRegression(x_train, y_train, x_test, y_test, learning_rate=3, num_iterations=300)
        self.assertIsInstance(test_accuracy, float)  # Check if the test accuracy is a float value

if __name__ == '__main__':
    unittest.main()

import unittest

# UNITTESTS

class TestImages(unittest.TestCase):

 # Tests velocityCalculator function
  def test_velocityCalculator(self):
    result = velocityCalculator((0,0), (3,4))
    self.assertEqual(result, 5.0/600)

    result = velocityCalculator((1,1), (2,2))
    self.assertEqual(result, (2**0.5)/600)

 # Tests relChangeFinder function
  def test_relChangeFinder(self):
    result = relChangeFinder(5,10)
    self.assertEqual(result, 1)

    result = relChangeFinder(-5,-7)
    self.assertEqual(result, 0.4)

unittest.main(argv=[''], verbosity=2, exit=False)